This document is a compilation of various things I thought were important to note down.

This format goes as follows
* each section starts after a line of /
* the text after a # denotes the title of the section


////////////////////////////////////////////////////////////////////////////////////////////////////


# general questions

can i use the same packing for SIMD CPU GEMM kernels for SIMT GPU GEMM kernels?

what need to be done to increase TC utilization?



////////////////////////////////////////////////////////////////////////////////////////////////////

# hardware properties

80 Volta MPs
8 TC per MP
2048 threads per MP
64 warps per MP
96 KB shared memory per MP
48 KB shared memory per thread blocks
4608 KB of L2 

1 TC does 4x4x4 mat-mul, this results in 2*4^3 flops = 2^7 flops per TC

1 volta can do TC_per_MP * MP_per_volta * FLOPS_per_TC = 8 * 80 * 2^7 = 81,920 FLOPS per volta

1 wmma does 16x16x16 mat-mul, this results in 2*16^3 flops = 2^13 flops per wmma

////////////////////////////////////////////////////////////////////////////////////////////////////

# deductions from hardware properties

equation to determine number of bytes per wmma


    A takes (m_wmma)(k_wmma)(S_data_in)
    B takes (n_wmma)(k_wmma)(S_data_in)
    C takes (m_wmma)(n_wmma)(S_data_out)

total = (m_wmma)(k_wmma)(S_data_in) + (n_wmma)(k_wmma)(S_data_in) + (m_wmma)(n_wmma)(S_data_out)
      = (k_wmma) (S_data_in) (m_wmma+n_wmma) + (m_wmma)(n_wmma)(S_data_out)

thus in shgemm we get

    total = 2*512 + 1024 = 2048 bytes per wmma in shgemm

total wmma calls 

(m*n*k) / (wmma_m * wmma_n * wmma_k)


////////////////////////////////////////////////////////////////////////////////////////////////////

# ideas on shgemm

my approach:

each thread in a warp will get the same address of the result matrix

therefore we need equations to calculate the address

assumptions:
split shared memory by half
1 warp = 1 block
32 threads = 1 warp

to increase TC utilization, we ...


////////////////////////////////////////////////////////////////////////////////////////////////////


# info pulled from cuda docs


An exception is the case where all threads in a warp address the same shared memory address, 
resulting in a broadcast.

The fraction of cycles the tensor (HMMA / IMMA) pipe was active. The value represents an average 
over a time interval and is not an instantaneous value. Higher values indicate higher utilization 
of the Tensor Cores. An activity of 1 (100%) is equivalent to issuing a tensor instruction every 
other cycle for the entire time interval. An activity of 0.2 (20%) could indicate 20% of the SMs 
are at 100% utilization over the entire time period, 100% of the SMs are at 20% utilization over 
the entire time period, 100% of the SMs are at 100% utilization for 20% of the time period, or any 
combination in between (see DCGM_FI_PROF_SM_ACTIVE to help disambiguate these possibilities).


////////////////////////////////////////////////////////////////////////////////////////////////////

# scratch pad 

an avg 0.000071842 seconds to compute a 720x720x720 shgemm

thus (2*720^3) / (0.000071842 * 10^12) = 10.4 TFLOPS

10.4 / 119 ~= 9% of peak



////////////////////////////////////////////////////////////////////////////////////////////////////

# general notes

1 warp gets 2 tensor cores
each tensore core can do a 4x4x4 mat mul
according to nvidia, 1 tensore core op will take 1 cycle
1 wmma will compute a 16x16x16 mat mul

each element represents a 4x4 matrix

                 B00 B01 B02 B03
                 B10 B11 B12 B13
                 B20 B21 B22 B23
                 B30 B31 B32 B33

A00 A01 A02 A03  C00 C01 C02 C03
A10 A11 A12 A13  C10 C11 C12 C13
A20 A21 A22 A23  C20 C21 C22 C23
A30 A31 A32 A33  C30 C31 C32 C33


- 4 seperate tensor calls for 1 tile of C
C00 += (A00)(B00) + (A01)(B10) + (A02)(B20) + (A03)(B30)

- each wmma instruction requires 4 * 16 tensor core calls
- assuming 